{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yahoo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/yahoo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/yahoo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/yahoo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/yahoo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/yahoo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/yahoo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/yahoo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/yahoo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/yahoo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/yahoo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/yahoo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Conv1D, Input, BatchNormalization, Add, GaussianNoise, Activation, Dropout, Dense, GlobalAveragePooling1D, Multiply\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data as expected\n",
    "Data can't be shared unfortunately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"train_lofi_rowid_Nov13.csv\"\n",
    "trainDF = pn.read_csv(train_path)\n",
    "final_test_path = \"test_lofi_rowid_Nov13.csv\"\n",
    "testDF = pn.read_csv(final_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "each x in X is a 1100 dimensional vector corresponding to the GR readings at each well depth. There will be 4000 entries in X.\n",
    "\n",
    "each y in Y is ground truth at that depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_gr = trainDF[['well_id', 'GR', 'label']].groupby('well_id').apply(lambda x: x['GR'])\n",
    "grouped_label = trainDF[['well_id', 'GR', 'label']].groupby('well_id').apply(lambda x: x['label'])\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i in range(0, 4000):\n",
    "    X.append(grouped_gr[i].values)\n",
    "    Y.append(grouped_label[i].values)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scale the Data\n",
    "Neural Nets usually benefit from subtracting the mean and dividing by the standard deviation. Technically, we should do this independently the for each validation split, but for now this is ok for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_std = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = to_categorical(Y, num_classes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "The model is is a resnet-like convolutional neural net, except that it is 1D instead of 2D and we do not use maxpooling, since the output dimension is the same as the input dimension. The input is 1100 GR readings and the output is 1100 predictions.\n",
    "\n",
    "The model architecture was discovered by trial and error -- making a model large enough to memorize the dataset (that is an indicator it has enough expresive power to model the problem) but not so large that it takes a long time to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    def SqueezeExcite(x, ratio=16):\n",
    "        nb_chan = K.int_shape(x)[-1]\n",
    "\n",
    "        y = GlobalAveragePooling1D()(x)\n",
    "        y = Dense(nb_chan // ratio, kernel_initializer='he_normal', activation='relu')(y)\n",
    "        y = Dense(nb_chan, kernel_initializer='he_normal', activation='sigmoid')(y)\n",
    "\n",
    "        y = Multiply()([x, y])\n",
    "        return y\n",
    "\n",
    "    inputs = Input((1100,1))\n",
    "    noised_inputs = GaussianNoise(0.01)(inputs)\n",
    "\n",
    "    CHANNELS = 64\n",
    "    FILTER_SIZE = 15\n",
    "    DROP_RATE = 0.1\n",
    "    \n",
    "    conv_x = Conv1D(CHANNELS, FILTER_SIZE, strides=1, padding='same', kernel_regularizer=l2(1e-5), kernel_initializer='he_normal')(noised_inputs)\n",
    "    conv_x = GaussianNoise(0.1)(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "    conv_x = BatchNormalization()(conv_x)\n",
    "    conv_x = Dropout(DROP_RATE)(conv_x)\n",
    "    for i in range(36):\n",
    "\n",
    "        conv_i = Conv1D(CHANNELS, FILTER_SIZE, strides=1, padding='same', kernel_regularizer=l2(1e-5), kernel_initializer='he_normal')(conv_x)\n",
    "        conv_i = GaussianNoise(0.1)(conv_i)\n",
    "        conv_i = Activation('relu')(conv_i)\n",
    "        conv_i = BatchNormalization()(conv_i)\n",
    "        conv_i = Dropout(DROP_RATE)(conv_i)\n",
    "        \n",
    "        conv_i = SqueezeExcite(conv_i)\n",
    "\n",
    "        conv_i = Conv1D(CHANNELS, FILTER_SIZE, strides=1, padding='same', kernel_regularizer=l2(1e-5), kernel_initializer='he_normal')(conv_i)\n",
    "        conv_i = GaussianNoise(0.1)(conv_i)\n",
    "        conv_i = Activation('relu')(conv_i)\n",
    "        conv_i = BatchNormalization()(conv_i)\n",
    "        conv_i = Dropout(DROP_RATE)(conv_i)\n",
    "\n",
    "        conv_i = SqueezeExcite(conv_i)\n",
    "        \n",
    "        conv_x = Add()([conv_x, conv_i])\n",
    "\n",
    "    conv_x = SqueezeExcite(conv_x)\n",
    "    conv_f = Conv1D(5, FILTER_SIZE, strides=1, padding='same', activation='softmax', kernel_regularizer=l2(1e-5), kernel_initializer='he_normal')(conv_x)\n",
    "\n",
    "\n",
    "    model = Model(input=inputs, output=conv_f)\n",
    "    \n",
    "    model.compile(optimizer = Adam(lr = 3e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do a k-fold Split\n",
    "It's standard practice to divide the dataset into _k_ partitions then train _k_ models, each having one of the partitions held out for validation. We will then ensemble the _k_ models together. That way, our final model has seen the entire dataset, but isn't overfitted to it.\n",
    "\n",
    "On a desktop running a GTX 1070, it takes about 34s / epoch and about 90 epochs before early stopping kicks in. For k=4, that translates to about 3.4 hours of training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yahoo/.local/lib/python3.6/site-packages/ipykernel_launcher.py:48: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/300\n",
      "3200/3200 [==============================] - 139s 43ms/step - loss: 0.2172 - accuracy: 0.9610 - val_loss: 0.2217 - val_accuracy: 0.9539\n",
      "Epoch 2/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.1452 - accuracy: 0.9822 - val_loss: 0.1494 - val_accuracy: 0.9790\n",
      "Epoch 3/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.1353 - accuracy: 0.9851 - val_loss: 0.1315 - val_accuracy: 0.9858\n",
      "Epoch 4/300\n",
      "3200/3200 [==============================] - 88s 27ms/step - loss: 0.1283 - accuracy: 0.9870 - val_loss: 0.1228 - val_accuracy: 0.9886\n",
      "Epoch 5/300\n",
      "3200/3200 [==============================] - 88s 27ms/step - loss: 0.1241 - accuracy: 0.9879 - val_loss: 0.1239 - val_accuracy: 0.9879\n",
      "Epoch 6/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.1198 - accuracy: 0.9888 - val_loss: 0.1220 - val_accuracy: 0.9882\n",
      "Epoch 7/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.1169 - accuracy: 0.9892 - val_loss: 0.1141 - val_accuracy: 0.9902\n",
      "Epoch 8/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.1137 - accuracy: 0.9898 - val_loss: 0.1154 - val_accuracy: 0.9895\n",
      "Epoch 9/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.1107 - accuracy: 0.9901 - val_loss: 0.1081 - val_accuracy: 0.9909\n",
      "Epoch 10/300\n",
      "3200/3200 [==============================] - 86s 27ms/step - loss: 0.1073 - accuracy: 0.9907 - val_loss: 0.1144 - val_accuracy: 0.9889\n",
      "Epoch 11/300\n",
      "3200/3200 [==============================] - 88s 28ms/step - loss: 0.1050 - accuracy: 0.9908 - val_loss: 0.1041 - val_accuracy: 0.9909\n",
      "Epoch 12/300\n",
      "3200/3200 [==============================] - 86s 27ms/step - loss: 0.1023 - accuracy: 0.9911 - val_loss: 0.1027 - val_accuracy: 0.9906\n",
      "Epoch 13/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0994 - accuracy: 0.9915 - val_loss: 0.1001 - val_accuracy: 0.9911\n",
      "Epoch 14/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0970 - accuracy: 0.9916 - val_loss: 0.0977 - val_accuracy: 0.9913\n",
      "Epoch 15/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0942 - accuracy: 0.9919 - val_loss: 0.0950 - val_accuracy: 0.9914\n",
      "Epoch 16/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0918 - accuracy: 0.9920 - val_loss: 0.0936 - val_accuracy: 0.9912\n",
      "Epoch 17/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0889 - accuracy: 0.9923 - val_loss: 0.0900 - val_accuracy: 0.9919\n",
      "Epoch 18/300\n",
      "3200/3200 [==============================] - 86s 27ms/step - loss: 0.0873 - accuracy: 0.9922 - val_loss: 0.0903 - val_accuracy: 0.9911\n",
      "Epoch 19/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0844 - accuracy: 0.9926 - val_loss: 0.0852 - val_accuracy: 0.9920\n",
      "Epoch 20/300\n",
      "3200/3200 [==============================] - 86s 27ms/step - loss: 0.0820 - accuracy: 0.9927 - val_loss: 0.0872 - val_accuracy: 0.9913\n",
      "Epoch 21/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0802 - accuracy: 0.9927 - val_loss: 0.0829 - val_accuracy: 0.9918\n",
      "Epoch 22/300\n",
      "3200/3200 [==============================] - 86s 27ms/step - loss: 0.0776 - accuracy: 0.9930 - val_loss: 0.0825 - val_accuracy: 0.9912\n",
      "Epoch 23/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0756 - accuracy: 0.9930 - val_loss: 0.0802 - val_accuracy: 0.9916\n",
      "Epoch 24/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0730 - accuracy: 0.9933 - val_loss: 0.0751 - val_accuracy: 0.9925\n",
      "Epoch 25/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0713 - accuracy: 0.9933 - val_loss: 0.0746 - val_accuracy: 0.9920\n",
      "Epoch 26/300\n",
      "3200/3200 [==============================] - 86s 27ms/step - loss: 0.0691 - accuracy: 0.9935 - val_loss: 0.0743 - val_accuracy: 0.9918\n",
      "Epoch 27/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0673 - accuracy: 0.9935 - val_loss: 0.0731 - val_accuracy: 0.9918\n",
      "Epoch 28/300\n",
      "3200/3200 [==============================] - 86s 27ms/step - loss: 0.0657 - accuracy: 0.9935 - val_loss: 0.0697 - val_accuracy: 0.9922\n",
      "Epoch 29/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0637 - accuracy: 0.9937 - val_loss: 0.0689 - val_accuracy: 0.9922\n",
      "Epoch 30/300\n",
      "3200/3200 [==============================] - 86s 27ms/step - loss: 0.0615 - accuracy: 0.9939 - val_loss: 0.0681 - val_accuracy: 0.9920\n",
      "Epoch 31/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0601 - accuracy: 0.9938 - val_loss: 0.0676 - val_accuracy: 0.9918\n",
      "Epoch 32/300\n",
      "3200/3200 [==============================] - 86s 27ms/step - loss: 0.0583 - accuracy: 0.9939 - val_loss: 0.0665 - val_accuracy: 0.9917\n",
      "Epoch 33/300\n",
      "3200/3200 [==============================] - 86s 27ms/step - loss: 0.0568 - accuracy: 0.9941 - val_loss: 0.0637 - val_accuracy: 0.9919\n",
      "Epoch 34/300\n",
      "3200/3200 [==============================] - 86s 27ms/step - loss: 0.0554 - accuracy: 0.9940 - val_loss: 0.0618 - val_accuracy: 0.9920\n",
      "Epoch 35/300\n",
      "3200/3200 [==============================] - 86s 27ms/step - loss: 0.0541 - accuracy: 0.9941 - val_loss: 0.0603 - val_accuracy: 0.9922\n",
      "Epoch 36/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0523 - accuracy: 0.9943 - val_loss: 0.0594 - val_accuracy: 0.9922\n",
      "Epoch 37/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0510 - accuracy: 0.9943 - val_loss: 0.0590 - val_accuracy: 0.9922\n",
      "Epoch 38/300\n",
      "3200/3200 [==============================] - 86s 27ms/step - loss: 0.0491 - accuracy: 0.9946 - val_loss: 0.0570 - val_accuracy: 0.9924\n",
      "Epoch 39/300\n",
      "3200/3200 [==============================] - 86s 27ms/step - loss: 0.0480 - accuracy: 0.9946 - val_loss: 0.0588 - val_accuracy: 0.9913\n",
      "Epoch 40/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0474 - accuracy: 0.9945 - val_loss: 0.0582 - val_accuracy: 0.9923\n",
      "Epoch 41/300\n",
      "3200/3200 [==============================] - 86s 27ms/step - loss: 0.0460 - accuracy: 0.9946 - val_loss: 0.0581 - val_accuracy: 0.9917\n",
      "Epoch 42/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0445 - accuracy: 0.9948 - val_loss: 0.0541 - val_accuracy: 0.9924\n",
      "Epoch 43/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0432 - accuracy: 0.9949 - val_loss: 0.0527 - val_accuracy: 0.9923\n",
      "Epoch 44/300\n",
      "3200/3200 [==============================] - 86s 27ms/step - loss: 0.0428 - accuracy: 0.9948 - val_loss: 0.0529 - val_accuracy: 0.9918\n",
      "Epoch 45/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0415 - accuracy: 0.9949 - val_loss: 0.0516 - val_accuracy: 0.9924\n",
      "Epoch 46/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0406 - accuracy: 0.9950 - val_loss: 0.0503 - val_accuracy: 0.9924\n",
      "Epoch 47/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0387 - accuracy: 0.9954 - val_loss: 0.0505 - val_accuracy: 0.9919\n",
      "Epoch 48/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0381 - accuracy: 0.9953 - val_loss: 0.0495 - val_accuracy: 0.9924\n",
      "Epoch 49/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0382 - accuracy: 0.9951 - val_loss: 0.0516 - val_accuracy: 0.9918\n",
      "Epoch 50/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0376 - accuracy: 0.9950 - val_loss: 0.0501 - val_accuracy: 0.9921\n",
      "Epoch 51/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0359 - accuracy: 0.9955 - val_loss: 0.0478 - val_accuracy: 0.9923\n",
      "Epoch 52/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0350 - accuracy: 0.9955 - val_loss: 0.0487 - val_accuracy: 0.9920\n",
      "Epoch 53/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0346 - accuracy: 0.9955 - val_loss: 0.0482 - val_accuracy: 0.9921\n",
      "Epoch 54/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0341 - accuracy: 0.9955 - val_loss: 0.0472 - val_accuracy: 0.9923\n",
      "Epoch 55/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0331 - accuracy: 0.9957 - val_loss: 0.0475 - val_accuracy: 0.9921\n",
      "Epoch 56/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0328 - accuracy: 0.9956 - val_loss: 0.0472 - val_accuracy: 0.9922\n",
      "Epoch 57/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0317 - accuracy: 0.9958 - val_loss: 0.0480 - val_accuracy: 0.9921\n",
      "Epoch 58/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0318 - accuracy: 0.9956 - val_loss: 0.0461 - val_accuracy: 0.9921\n",
      "Epoch 59/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0303 - accuracy: 0.9960 - val_loss: 0.0450 - val_accuracy: 0.9925\n",
      "Epoch 60/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0300 - accuracy: 0.9960 - val_loss: 0.0461 - val_accuracy: 0.9920\n",
      "Epoch 61/300\n",
      "3200/3200 [==============================] - 88s 28ms/step - loss: 0.0292 - accuracy: 0.9961 - val_loss: 0.0463 - val_accuracy: 0.9922\n",
      "Epoch 62/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0287 - accuracy: 0.9961 - val_loss: 0.0460 - val_accuracy: 0.9923\n",
      "Epoch 63/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0283 - accuracy: 0.9962 - val_loss: 0.0496 - val_accuracy: 0.9909\n",
      "Epoch 64/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0288 - accuracy: 0.9959 - val_loss: 0.0451 - val_accuracy: 0.9920\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 6.000000284984708e-05.\n",
      "Epoch 65/300\n",
      "3200/3200 [==============================] - 88s 27ms/step - loss: 0.0251 - accuracy: 0.9972 - val_loss: 0.0427 - val_accuracy: 0.9930\n",
      "Epoch 66/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0236 - accuracy: 0.9977 - val_loss: 0.0435 - val_accuracy: 0.9930\n",
      "Epoch 67/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0230 - accuracy: 0.9978 - val_loss: 0.0440 - val_accuracy: 0.9930\n",
      "Epoch 68/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0225 - accuracy: 0.9979 - val_loss: 0.0441 - val_accuracy: 0.9930\n",
      "Epoch 69/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0222 - accuracy: 0.9980 - val_loss: 0.0450 - val_accuracy: 0.9930\n",
      "Epoch 70/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0218 - accuracy: 0.9980 - val_loss: 0.0453 - val_accuracy: 0.9930\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 1.2000000424450263e-05.\n",
      "Epoch 71/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0214 - accuracy: 0.9982 - val_loss: 0.0452 - val_accuracy: 0.9931\n",
      "Epoch 72/300\n",
      "3200/3200 [==============================] - 86s 27ms/step - loss: 0.0212 - accuracy: 0.9982 - val_loss: 0.0453 - val_accuracy: 0.9930\n",
      "Epoch 73/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0211 - accuracy: 0.9983 - val_loss: 0.0455 - val_accuracy: 0.9930\n",
      "Epoch 74/300\n",
      "3200/3200 [==============================] - 88s 27ms/step - loss: 0.0211 - accuracy: 0.9983 - val_loss: 0.0456 - val_accuracy: 0.9931\n",
      "Epoch 75/300\n",
      "3200/3200 [==============================] - 87s 27ms/step - loss: 0.0210 - accuracy: 0.9983 - val_loss: 0.0457 - val_accuracy: 0.9930\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 2.4000000848900527e-06.\n",
      "Epoch 76/300\n",
      "3200/3200 [==============================] - 88s 27ms/step - loss: 0.0209 - accuracy: 0.9983 - val_loss: 0.0458 - val_accuracy: 0.9931\n",
      "Epoch 77/300\n",
      "3200/3200 [==============================] - 88s 27ms/step - loss: 0.0208 - accuracy: 0.9983 - val_loss: 0.0458 - val_accuracy: 0.9931\n",
      "CPU times: user 2h 16min 6s, sys: 21min 42s, total: 2h 37min 49s\n",
      "Wall time: 1h 55min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "models_dumb = []\n",
    "kf = KFold(n_splits=5)\n",
    "counter = 1\n",
    "for train_index, test_index in kf.split(X_std):\n",
    "    if counter <= 4:\n",
    "        counter += 1\n",
    "        continue\n",
    "    X_train, X_test = X_std[train_index], X_std[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=12)\n",
    "\n",
    "    model = get_model()\n",
    "    model.fit(x=np.expand_dims(X_train, axis=-1), y=y_train, validation_data=(np.expand_dims(X_test, axis=-1), y_test), epochs=300, callbacks=[reduce_lr, early_stopping])\n",
    "    models_dumb.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7fe95c924f28>,\n",
       " <keras.engine.training.Model at 0x7fe86a4dc9b0>,\n",
       " <keras.engine.training.Model at 0x7fe614ea74a8>,\n",
       " <keras.engine.training.Model at 0x7fe5ec986a20>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7fe599e86780>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dumb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 3 µs, total: 5 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def predict_on_data(data):\n",
    "    results = []\n",
    "    for model in models + models_dumb:\n",
    "        print(\"predict...\")\n",
    "        result = model.predict(np.expand_dims(data, axis=-1))\n",
    "        results.append(result)\n",
    "\n",
    "    # Average the results together, then take the index of the highest value\n",
    "    return np.argmax(sum(results) / len(models), axis=-1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_well_ids = testDF.well_id.unique()\n",
    "X_test = []\n",
    "for well_id in test_well_ids:\n",
    "    GR = testDF[testDF.well_id == well_id].GR.values\n",
    "    X_test.append(GR)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.034528409659228175"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict...\n",
      "predict...\n",
      "predict...\n",
      "predict...\n",
      "predict...\n"
     ]
    }
   ],
   "source": [
    "submission_values = predict_on_data(X_test_std)\n",
    "np.save('submission_values.npy', submission_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([202462.,      0.,  45135.,      0.,      0.,  45508.,      0.,\n",
       "         45253.,      0.,  46642.]),\n",
       " array([0. , 0.4, 0.8, 1.2, 1.6, 2. , 2.4, 2.8, 3.2, 3.6, 4. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(submission_values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 1100)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF[\"PredictedLabel\"] = submission_values.flatten()\n",
    "testDF[[\"row_id\",\"well_id\",\"PredictedLabel\"]].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>well_id</th>\n",
       "      <th>GR</th>\n",
       "      <th>PredictedLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5000</td>\n",
       "      <td>134.943504</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>127.004675</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5000</td>\n",
       "      <td>133.159255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5000</td>\n",
       "      <td>134.411762</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5000</td>\n",
       "      <td>135.748644</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384995</th>\n",
       "      <td>1095</td>\n",
       "      <td>5349</td>\n",
       "      <td>134.221769</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384996</th>\n",
       "      <td>1096</td>\n",
       "      <td>5349</td>\n",
       "      <td>135.804491</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384997</th>\n",
       "      <td>1097</td>\n",
       "      <td>5349</td>\n",
       "      <td>126.124399</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384998</th>\n",
       "      <td>1098</td>\n",
       "      <td>5349</td>\n",
       "      <td>117.591583</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384999</th>\n",
       "      <td>1099</td>\n",
       "      <td>5349</td>\n",
       "      <td>132.555150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>385000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        row_id  well_id          GR  PredictedLabel\n",
       "0            0     5000  134.943504               0\n",
       "1            1     5000  127.004675               0\n",
       "2            2     5000  133.159255               0\n",
       "3            3     5000  134.411762               0\n",
       "4            4     5000  135.748644               0\n",
       "...        ...      ...         ...             ...\n",
       "384995    1095     5349  134.221769               0\n",
       "384996    1096     5349  135.804491               0\n",
       "384997    1097     5349  126.124399               0\n",
       "384998    1098     5349  117.591583               0\n",
       "384999    1099     5349  132.555150               0\n",
       "\n",
       "[385000 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
